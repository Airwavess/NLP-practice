{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "import re\n",
    "\n",
    "import jieba\n",
    "import jieba.analyse\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "jieba.load_userdict(\"./dict.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeans:\n",
    "    def cal_dist(self, p0, p1):\n",
    "        \"\"\"\n",
    "        比較兩點的距離\n",
    "        \"\"\"\n",
    "        return np.sqrt(np.sum((p0-p1)**2))\n",
    "\n",
    "    def kmeans(self, datapoints, k=2):\n",
    "        # 定義資料維度\n",
    "        d = datapoints.shape[1]\n",
    "        # 最大的迭代次數\n",
    "        Max_Iterations = 1000\n",
    "\n",
    "        cluster = np.zeros(datapoints.shape[0])\n",
    "        prev_cluster = np.ones(datapoints.shape[0])\n",
    "\n",
    "        cluster_centers = []\n",
    "        for i in range(k):\n",
    "            cluster_centers += [random.choice(datapoints)]\n",
    "\n",
    "        iteration = 0\n",
    "        while np.array_equal(cluster, prev_cluster) is False or iteration > Max_Iterations:\n",
    "            iteration += 1\n",
    "            prev_cluster = cluster.copy()\n",
    "\n",
    "            # 將每一個點做分群\n",
    "            for idx, point in enumerate(datapoints):\n",
    "                min_dist = float(\"inf\")\n",
    "                for c, cluster_center in enumerate(cluster_centers):\n",
    "                    dist = self.cal_dist(point, cluster_center)\n",
    "                    if dist < min_dist:\n",
    "                        min_dist = dist  \n",
    "                        cluster[idx] = c   # 指定該點屬於哪個分群\n",
    "\n",
    "            # 更新分群的中心\n",
    "            for k in range(len(cluster_centers)):\n",
    "                new_center = np.zeros(d)\n",
    "                members = 0\n",
    "                for point, c in zip(datapoints, cluster):\n",
    "                    if c == k:\n",
    "                        new_center += point\n",
    "                        members += 1\n",
    "                if members > 0:\n",
    "                    new_center = new_center / members\n",
    "                cluster_centers[k] = new_center\n",
    "\n",
    "        return cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 讀取資料\n",
    "DATASET_DIR = './speech.json'\n",
    "with open(DATASET_DIR) as f:\n",
    "    dataset = json.load(f)\n",
    "    \n",
    "# 讀取 stop words\n",
    "STOP_WORDS_DIR = './stop_words.txt'\n",
    "with open(STOP_WORDS_DIR) as f:\n",
    "    stop_words = f.read().splitlines() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 讀取演講內容與縣市\n",
    "speech_list = list(map(lambda d: d['speech'], dataset))\n",
    "country_list = list(map(lambda d: d['country'], dataset))\n",
    "\n",
    "# 去除繁體中文以外的英文、數字、符號\n",
    "rule = re.compile(r\"[^\\u4e00-\\u9fa5]\")\n",
    "speech_list = [list(jieba.cut(rule.sub('', speech))) for speech in speech_list]\n",
    "for idx, speech in enumerate(speech_list):\n",
    "    speech_list[idx] = ' '.join([word for word in speech if word not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解析最每一篇演講中會相關的字\n",
    "vectorizer = CountVectorizer()\n",
    "transformer = TfidfTransformer()\n",
    "tfidf = transformer.fit_transform(vectorizer.fit_transform(speech_list))\n",
    "bag_of_words = vectorizer.get_feature_names()\n",
    "weight = tfidf.toarray()\n",
    "\n",
    "news_most_related_words = {}\n",
    "for i in range(len(weight)): \n",
    "    w = dict(zip(bag_of_words, weight[i]))\n",
    "    w = sorted(w.items(), key=lambda x: x[1], reverse=True)\n",
    "    top_10 = []\n",
    "    for word, prob in w[:10]:\n",
    "        if prob > 0:\n",
    "            top_10.append(word)\n",
    "    news_most_related_words.update({country_list[i]: top_10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'台北市': ['上工', '同仁', '工作'],\n",
       " '新北市': ['新北市', '市民', '侯友宜', '解決', '團隊', '政治', '希望', '便民', '朱市長', '正氣'],\n",
       " '桃園市': ['桃園', '程式', '建設', '發展', '政治', '好不好', '城市', '立委', '特別', '希望'],\n",
       " '高雄市': ['高雄市', '高雄', '春天', '轟動', '城市', '三十年', '世界', '全世界', '包容', '市府'],\n",
       " '新竹市': ['新竹市', '市民', '非常', '城市', '四年', '新竹', '市長', '加倍', '朋友', '參議'],\n",
       " '嘉義市': ['謝謝', '朋友', '力量', '善良', '能量', '告訴', '支持', '嘉義', '嘉義市', '過程'],\n",
       " '雲林縣': ['雲林', '鄉親', '發展', '上場', '失衡', '數位', '自我', '雲林良品', '面對', '品牌']}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_most_related_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將字詞向量化 BOW\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform([' '.join(w) for w in list(news_most_related_words.values())]) \n",
    "X = X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 107)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0: 新竹市\n",
      "Cluster 1: 桃園市\n",
      "Cluster 2: 台北市\n",
      "Cluster 3: 新北市 高雄市 嘉義市 雲林縣\n"
     ]
    }
   ],
   "source": [
    "k = 4\n",
    "K = KMeans()\n",
    "speech_cluster_result = K.kmeans(X, k)\n",
    "cluster = [[] for _ in range(k)]\n",
    "\n",
    "for idx, c in enumerate(speech_cluster_result):\n",
    "    cluster[int(c)].append(country_list[idx])\n",
    "    \n",
    "for c, result in enumerate(cluster):\n",
    "    print('Cluster {}: {}'.format(c, ' '.join(result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
