{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "import re\n",
    "\n",
    "import jieba\n",
    "import jieba.analyse\n",
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "jieba.load_userdict(\"./dict.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeans:\n",
    "    def cal_dist(self, p0, p1):\n",
    "        \"\"\"\n",
    "        比較兩點的距離\n",
    "        \"\"\"\n",
    "#         return cosine_similarity([p0], [p1])[0][0]\n",
    "        return np.sqrt(np.sum((p0-p1)**2))\n",
    "    \n",
    "    def nearest_cluster_center(self, point, cluster_centers):\n",
    "        \"\"\"\n",
    "        找到距離 point 最近的中心點\n",
    "        \"\"\"\n",
    "        min_dist = float(\"inf\")\n",
    "        m = cluster_centers.shape[0]\n",
    "        for i in range(m):\n",
    "            d = self.cal_dist(point, cluster_centers[i])\n",
    "            if min_dist > d:\n",
    "                min_dist = d\n",
    "        return min_dist \n",
    "\n",
    "    def get_centroids(self, datapoints, k):\n",
    "        \"\"\"\n",
    "        K-means++ 演算法，取得初始化中心點\n",
    "        \"\"\"\n",
    "        clusters = np.array([random.choice(datapoints)])\n",
    "        dist = np.zeros(len(datapoints))\n",
    "        \n",
    "        for i in range(k):\n",
    "            sum_dist = 0\n",
    "            for j, point in enumerate(datapoints):\n",
    "                dist[j] = self.nearest_cluster_center(point, clusters)\n",
    "                sum_dist += dist[j]\n",
    "            \n",
    "            sum_dist *= random.random()\n",
    "            for j, d in enumerate(dist):\n",
    "                sum_dist = sum_dist - d\n",
    "                if sum_dist <= 0:\n",
    "                    clusters = np.append(clusters, [datapoints[j]], axis=0)\n",
    "                    break\n",
    "        \n",
    "        return clusters\n",
    "        \n",
    "        \n",
    "    def kmeans_plus_plus(self, datapoints, k=2):\n",
    "        \"\"\"\n",
    "        K-means 演算法\n",
    "        \"\"\"\n",
    "        # 定義資料維度\n",
    "        d = datapoints.shape[1]\n",
    "        # 最大的迭代次數\n",
    "        Max_Iterations = 1000\n",
    "\n",
    "        cluster = np.zeros(datapoints.shape[0])\n",
    "        prev_cluster = np.ones(datapoints.shape[0])\n",
    "\n",
    "        cluster_centers = self.get_centroids(datapoints, k)\n",
    "\n",
    "        iteration = 0\n",
    "        while np.array_equal(cluster, prev_cluster) is False or iteration > Max_Iterations:\n",
    "            iteration += 1\n",
    "            prev_cluster = cluster.copy()\n",
    "\n",
    "            # 將每一個點做分群\n",
    "            for idx, point in enumerate(datapoints):\n",
    "                min_dist = float(\"inf\")\n",
    "                for c, cluster_center in enumerate(cluster_centers):\n",
    "                    dist = self.cal_dist(point, cluster_center)\n",
    "                    if dist < min_dist:\n",
    "                        min_dist = dist  \n",
    "                        cluster[idx] = c   # 指定該點屬於哪個分群\n",
    "\n",
    "            # 更新分群的中心\n",
    "            for k in range(len(cluster_centers)):\n",
    "                new_center = np.zeros(d)\n",
    "                members = 0\n",
    "                for point, c in zip(datapoints, cluster):\n",
    "                    if c == k:\n",
    "                        new_center += point\n",
    "                        members += 1\n",
    "                if members > 0:\n",
    "                    new_center = new_center / members\n",
    "                cluster_centers[k] = new_center\n",
    "\n",
    "        return cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 讀取資料\n",
    "DATASET_DIR = './speech.json'\n",
    "with open(DATASET_DIR, encoding = 'utf8') as f:\n",
    "    dataset = json.load(f)\n",
    "    \n",
    "# 讀取 stop words\n",
    "STOP_WORDS_DIR = './stop_words.txt'\n",
    "with open(STOP_WORDS_DIR, encoding = 'utf8') as f:\n",
    "    stop_words = f.read().splitlines() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 讀取演講內容與縣市\n",
    "speech_list = list(map(lambda d: d['speech'], dataset))\n",
    "country_list = list(map(lambda d: d['country'], dataset))\n",
    "\n",
    "# 去除繁體中文以外的英文、數字、符號\n",
    "rule = re.compile(r\"[^\\u4e00-\\u9fa5]\")\n",
    "speech_list = [list(jieba.cut(rule.sub('', speech))) for speech in speech_list]\n",
    "for idx, speech in enumerate(speech_list):\n",
    "    speech_list[idx] = ' '.join([word for word in speech if word not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf = tfidf_vectorizer.fit_transform(speech_list)\n",
    "tfidf = tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-b1a364e165db>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mKmeans_cluster\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mspeech_cluster_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKmeans_cluster\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkmeans_plus_plus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfidf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mcluster\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-cfce827e546a>\u001b[0m in \u001b[0;36mkmeans_plus_plus\u001b[1;34m(self, datapoints, k)\u001b[0m\n\u001b[0;32m     64\u001b[0m                 \u001b[0mmin_dist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"inf\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcluster_center\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcluster_centers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m                     \u001b[0mdist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcal_dist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpoint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcluster_center\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mdist\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mmin_dist\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m                         \u001b[0mmin_dist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdist\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-cfce827e546a>\u001b[0m in \u001b[0;36mcal_dist\u001b[1;34m(self, p0, p1)\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0m比較兩點的距離\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \"\"\"\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mp0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mp1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mnearest_cluster_center\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpoint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcluster_centers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\u001b[0m in \u001b[0;36mcosine_similarity\u001b[1;34m(X, Y, dense_output)\u001b[0m\n\u001b[0;32m    896\u001b[0m     \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_pairwise_arrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 898\u001b[1;33m     \u001b[0mX_normalized\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    899\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    900\u001b[0m         \u001b[0mY_normalized\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_normalized\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py\u001b[0m in \u001b[0;36mnormalize\u001b[1;34m(X, norm, axis, copy, return_norm)\u001b[0m\n\u001b[0;32m   1578\u001b[0m             \u001b[0mnorms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1579\u001b[0m         \u001b[0mnorms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_handle_zeros_in_scale\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnorms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1580\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m/=\u001b[0m \u001b[0mnorms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1581\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1582\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "k = 3\n",
    "Kmeans_cluster = KMeans()\n",
    "speech_cluster_result = Kmeans_cluster.kmeans_plus_plus(tfidf, k)\n",
    "cluster = [[] for _ in range(k)]\n",
    "\n",
    "for idx, c in enumerate(speech_cluster_result):\n",
    "    cluster[int(c)].append(country_list[idx])\n",
    "    \n",
    "for c, result in enumerate(cluster):\n",
    "    print('Cluster {}: {}'.format(c, ' '.join(result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
